---
title: 'Analyzing bbcnews text'
Author: Ziyi
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: false
    number_sections: true
    code_folding: hide
---

### Step 0: loading data
```{r set up}
# load the packages
library(tm)
library(dplyr)
library(tidytext)
library(kernlab)
library(tidyr)
library(LaplacesDemon)
library(knitr)

busi_location <- "../data/bbc/business/"
entertain_location <- "../data/bbc/entertainment/"
politics_location <- "../data/bbc/politics/"
sport_location <- "../data/bbc/sport/"
tech_location <- "../data/bbc/tech/"
```

```{r read txt}
## a function for reading data
read_txt <- function(file_name, file_loca){
  current_file_name <- sub(".txt","",file_name)
  current_ground_truth <- readLines(paste(file_loca,current_file_name,".txt",sep=""), encoding="UTF-8",warn=FALSE)
  return(current_ground_truth)
}

busi_bigr_lib <- busi_location %>% list.files() %>% lapply(., read_txt,busi_location)
entertain_bigr_lib <- entertain_location %>% list.files() %>%
  lapply(., read_txt,entertain_location)
politics_bigr_lib <- politics_location %>%list.files() %>% 
  lapply(., read_txt,politics_location)
sport_bigr_lib <- sport_location %>% list.files() %>% lapply(., read_txt, sport_location)
tech_bigr_lib <- tech_location %>% list.files() %>% lapply(., read_txt, tech_location)
```


### Step 1: Extract features
```{r stop words}
## update the stop words
data("stop_words")
word <- c("bn","ago","yesterday","lot","today","months","month",
                 "happier","happiest","last","week","past")
# add words in the stop_words and the level of lexicon is 'updated
stop_words <- stop_words %>% bind_rows(mutate(tibble(word), lexicon = "updated")) 
```

```{r dictionary}
## a function for creating a corpus
dictionary <- function(bigr_lib){
  
  # creating a corpus
  corpus<- VCorpus(VectorSource(bigr_lib))%>%
    tm_map(content_transformer(tolower))%>%  # to lower case letters
    tm_map(removePunctuation)%>%             # remove punctuations
    tm_map(removeNumbers)%>%                 # remove numbers  
    tm_map(removeWords, character(0))%>%     # remove character(0)
    tm_map(stripWhitespace)                  # remove extra whitespace
  
  # tidy the corpus
  dict <- tidy(corpus) %>% select(text)  
  

  completed <- dict %>%
    unnest_tokens(dictionary, text) %>%
    anti_join(stop_words,by = c("dictionary" = "word"))  # remove stop words
  
  # return the dictionary
  list <- completed$dictionary %>% paste(., collapse = " ")
  return(list)
}
```

```{r biletter}
## a function to get the character bigram from a token
biletter <- function(token){
  # Split into characterss:
  w <- strsplit(token, "")[[1]]
  # character bigrams pasted together:
  return(vapply(ngrams(w, 2L), paste, "", collapse = ""))
}

## a function to apply the biletter function in a corpus
biletter_list <- function(corpus){
  letter <- list()
  for(i in 1:length(corpus)){
    letter[[i]]<- corpus[i] %>% as.character() %>% biletter()
  }
  letter <- unlist(letter)
  return(letter)
}
```

```{r}
## a function to extract the density of character bigram 
density <- function(reference){
  trans.mat=matrix(0,27,27)
  rownames(trans.mat)=colnames(trans.mat)=c(tolower(letters),"")
  lastletter=""
  
  for (ln in 1:length(reference)) {
    if (ln %% 1000 ==0) {cat("Line",ln,"\n")}
    for (pos in 1:nchar(reference[ln])) {
      curletter=substring(reference[ln],pos,pos)
      
      if (curletter %in% tolower(letters)) {
        trans.mat[rownames(trans.mat)==lastletter,
                  colnames(trans.mat)==curletter]=
          trans.mat[rownames(trans.mat)==lastletter,
                    colnames(trans.mat)==curletter]+1
        lastletter=curletter
      } else {
        
        if (lastletter!="") {
          trans.mat[rownames(trans.mat)==lastletter,27]=
            trans.mat[rownames(trans.mat)==lastletter,27]+1
          lastletter=""
        }
      }
    }
    curletter=""
    
    if (lastletter!="") {
      trans.mat[rownames(trans.mat)==lastletter,27]=
        trans.mat[rownames(trans.mat)==lastletter,27]+1
    }
    lastletter=""
  }
  
  trans.prob.mat <- (trans.mat+1) / sum(trans.mat)
  all_letters <- expand.grid(colnames(trans.prob.mat),rownames(trans.prob.mat)) %>%
    unite(., letters, c(Var1, Var2), remove=T, sep = "") %>% as.matrix() %>% as.vector()
  freq <- as.vector(trans.prob.mat)
  names(freq) <- all_letters
  return(freq)
}
```

```{r}
## a function to obtain the density in a list of files
extract.features <- function(bigr_lib,k = NA){
  features <- matrix(NA, nrow = length(bigr_lib), ncol = 729)
  for(i in 1:length(bigr_lib)){
    features[i,] <- bigr_lib[[i]] %>% dictionary() %>% density()
    colnames(features) <- names(b)
  }
  labels <- rep(k, length(bigr_lib))
  return(list(features = features, labels = labels))
}
```


```{r feature extraction}

## extract the features in each topic
busi_feat <- busi_bigr_lib %>% lapply(., dictionary)  %>% extract.features(.,k = "business")
sport_feat <- sport_bigr_lib %>% lapply(., dictionary) %>%extract.features(.,k = "sport")
entertain_feat <- entertain_bigr_lib %>% lapply(., dictionary) %>% extract.features(.,k = "entertainment")
politics_feat <- politics_bigr_lib %>% lapply(., dictionary) %>%extract.features(.,k = "politics")
tech_feat <- tech_bigr_lib %>% lapply(., dictionary) %>%extract.features(.,k = "technology")

```

```{r training ind}
## a function to get the training index
train.ind <- function(bigr_lib){
  ind <- sample(1:length(bigr_lib), floor(length(bigr_lib)*0.7), replace = F)
  return(ind)
}
```

```{r training set}
## 70% data as train set
set.seed(0)
busi_ind <- train.ind(busi_bigr_lib) # 357 files
politics_ind <- train.ind(politics_bigr_lib) # 291 files
entertain_ind <- train.ind(entertain_bigr_lib) # 270 files
sport_ind <- train.ind(sport_bigr_lib) # 357 files
tech_ind <- train.ind(tech_bigr_lib) # 280 files

## the feactures in each file in training set or test set
train_feat <- list(features = rbind(busi_feat$features[busi_ind,],#421086 character bigrams
                      politics_feat$features[politics_ind,], #438395 character bigrams
                      entertain_feat$features[entertain_ind,],  #299454 character bigrams
                      sport_feat$features[sport_ind,], #367672 character bigrams
                      tech_feat$features[tech_ind,]),  #470678 character bigrams
                   labels = c(busi_feat$labels[busi_ind], 
                              politics_feat$labels[politics_ind],
                              entertain_feat$labels[entertain_ind],
                              sport_feat$labels[sport_ind],
                              tech_feat$labels[tech_ind]))

test_feat <- list(features = rbind(busi_feat$features[-busi_ind,], #179682 character bigrams
                      politics_feat$features[-politics_ind,], #189993 character bigrams
                      entertain_feat$features[-entertain_ind,], #131791 character bigrams
                      sport_feat$features[-sport_ind,], #162229 character bigrams 
                      tech_feat$features[-tech_ind,]),  #202938 character bigrams
                   labels = c(busi_feat$labels[-busi_ind], 
                              politics_feat$labels[-politics_ind],
                              entertain_feat$labels[-entertain_ind],
                              sport_feat$labels[-sport_ind],
                              tech_feat$labels[-tech_ind]))


## combine the training set as one document and obtain the density
busi_train <- list.files(busi_location)[busi_ind] %>% 
  lapply(., read_txt,busi_location) %>% 
  dictionary() %>% toString() %>% density()

sport_train <- list.files(sport_location)[sport_ind] %>%
  lapply(., read_txt,sport_location) %>% 
  dictionary() %>% toString() %>% density()

politics_train <- list.files(politics_location)[politics_ind] %>% 
  lapply(., read_txt,politics_location) %>% dictionary() %>%
  toString() %>% density()

entertain_train <- list.files(entertain_location)[entertain_ind] %>% 
  lapply(., read_txt,entertain_location) %>% dictionary() %>%
  toString() %>% density()

tech_train <- list.files(tech_location)[tech_ind] %>% 
  lapply(., read_txt,tech_location) %>% dictionary() %>%
  toString() %>% density()
```

number of training tokens
```{r number of training tokens}
# business tokens
list.files(busi_location)[busi_ind] %>% 
  lapply(., read_txt,busi_location) %>% 
  dictionary() %>% biletter_list() %>% length()
# sport tokens
 list.files(sport_location)[sport_ind] %>%
  lapply(., read_txt,sport_location) %>% 
  dictionary() %>% biletter_list() %>% length()
# politics tokens
list.files(politics_location)[politics_ind] %>% 
  lapply(., read_txt,politics_location) %>% dictionary() %>%
  biletter_list() %>% length()
# entertainment tokens
list.files(entertain_location)[-entertain_ind] %>% 
  lapply(., read_txt,entertain_location) %>% dictionary() %>%
  biletter_list() %>% length()
# technology tokens
list.files(tech_location)[tech_ind] %>% 
  lapply(., read_txt,tech_location) %>% dictionary() %>%
  biletter_list() %>% length()
```
number of test tokens
```{r number of test tokens}
# business tokens
list.files(busi_location)[-busi_ind] %>% 
  lapply(., read_txt,busi_location) %>% 
  dictionary() %>% biletter_list() %>% length()
# sport tokens
 list.files(sport_location)[-sport_ind] %>%
  lapply(., read_txt,sport_location) %>% 
  dictionary() %>% biletter_list() %>% length()
# politics tokens
list.files(politics_location)[-politics_ind] %>% 
  lapply(., read_txt,politics_location) %>% dictionary() %>%
  biletter_list() %>% length()
# entertainment tokens
list.files(entertain_location)[-entertain_ind] %>% 
  lapply(., read_txt,entertain_location) %>% dictionary() %>%
  biletter_list() %>% length()
# technology tokens
list.files(tech_location)[-tech_ind] %>% 
  lapply(., read_txt,tech_location) %>% dictionary() %>%
  biletter_list() %>% length()
```

### Step 3: KLD
```{r}
## a function to obtain the prediction using KLD distance
KLD.label <- function(file){
  
  # KLD distance between file and business files
  kld_busi <- KLD(busi_train,file)$sum.KLD.px.py/KLD(busi_train,rep(0,729))$sum.KLD.px.py
  
  # KLD distance between file and politics files
  kld_politics <- KLD(politics_train,file)$sum.KLD.px.py/KLD(politics_train,rep(0,729))$sum.KLD.px.py
  
  # KLD distance between file and entertainment files
  kld_entertain <- KLD(entertain_train,file)$sum.KLD.px.py/KLD(entertain_train,rep(0,729))$sum.KLD.px.py
  
  # KLD distance between file and sport files
  kld_sport <- KLD(sport_train,file)$sum.KLD.px.py/KLD(sport_train,rep(0,729))$sum.KLD.px.py
  
  # KLD distance between file and technology files
  kld_tech <- KLD(tech_train,file)$sum.KLD.px.py/KLD(tech_train,rep(0,729))$sum.KLD.px.py
  
  # predict the label with lowest KLD distance
  kld.dist <- cbind(kld_busi, kld_politics, kld_entertain, kld_sport, kld_tech)
  labels <- c("business", "politics", "entertainment","sport","technology")
  
  ind <- which.min(kld.dist)
  return(labels[ind])
}

```

```{r training set KLD}

## classify training set with KLD distance
kld_train_pred <- rep(NA, nrow(train_feat$features))
for(i in 1:length(train_feat$labels)){
  kld_train_pred[i] <-   KLD.label(train_feat$features[i,])
}

## training error
kld_train_err <- sum(kld_train_pred != train_feat$labels)/length(train_feat$labels)
cat("KLD training error:", kld_train_err)

```

```{r test set KLD}

## classify test set with KLD distance
kld_test_pred <- rep(NA, nrow(test_feat$features))
for(i in 1:length(test_feat$labels)){
  kld_test_pred[i] <-   KLD.label(test_feat$features[i,])
}

## test error
kld_test_err <- sum(kld_test_pred != test_feat$labels)/length(test_feat$labels)
cat("KLD test error:", kld_test_err)

```

### Step 4: SVM
```{r SVM}
set.seed(2)

## fit the model
fit_svm <- ksvm(train_feat$features, as.factor(train_feat$labels), type = "C-svc")

## predict in test set
svm_test_pred <- predict(fit_svm, test_feat$features)

## results
svm_test_error <- sum(svm_test_pred != as.factor(test_feat$labels))/length(test_feat$labels)
cat("SVM training error:",fit_svm@error, "\n","SVM test error:", svm_test_error)

```

### Step 5: Test with a new topic

```{r}
## using just politics, entertainment, sport and tech as known topics
## treating business as the unknown topic that we need to add

## a function to classify with a new topic
KLD.label.new <- function(file){
  
  kld_busi <- KLD(dic,file)$sum.KLD.px.py/KLD(dic,rep(0,729))$sum.KLD.px.py
  kld_politics <- KLD(politics_train,file)$sum.KLD.px.py/KLD(politics_train,rep(0,729))$sum.KLD.px.py
  kld_entertain <- KLD(entertain_train,file)$sum.KLD.px.py/KLD(entertain_train,rep(0,729))$sum.KLD.px.py
  kld_sport <- KLD(sport_train,file)$sum.KLD.px.py/KLD(sport_train,rep(0,729))$sum.KLD.px.py
  kld_tech <- KLD(tech_train,file)$sum.KLD.px.py/KLD(tech_train,rep(0,729))$sum.KLD.px.py

  kld.dist <- cbind(kld_busi, kld_politics, kld_entertain, kld_sport, kld_tech)
  labels <- c("business", "politics", "entertainment","sport","technology")
  
  ind <- which.min(kld.dist)
  return(labels[ind])
}

```

```{r}
## using just one file in business topic to generate a topic density
file <- readLines("../data/bbc/business/001.txt") %>% strsplit(., split = " ") %>% unlist()
L <- length(file)
S <- 100
boots <- list()

for(s in 1:S){
  indices <- sample(1:L, L, replace = T)
  boots[[s]] <- file[indices]
}

boots <- boots %>% unlist() %>% dictionary() 
dic <- boots %>% density()
```

```{r}
## comparing results with using all the documents previously
busi_test <- list(features =busi_feat$features[-busi_ind,], labels = busi_feat$labels[-busi_ind])
kld_busi_pre <- rep(NA, nrow(busi_test$features))
for(i in 1:length(busi_test$labels)){
  kld_busi_pre[i] <-   KLD.label(busi_test$features[i,])
}

kld_busi_new <- rep(NA, nrow(busi_test$features))
for(i in 1:length(busi_test$labels)){
  kld_busi_new[i] <-   KLD.label.new(busi_test$features[i,])
}

kld_err_pre <- sum(kld_busi_pre != busi_test$labels)/length(busi_test$labels)
kld_err_new <- sum(kld_busi_new != busi_test$labels)/length(busi_test$labels)
cat("KLD error (previous):", kld_err_pre, "\n", "KLD error (new):", kld_err_new)
```


### Step 6: Comparing results
*KLD accuracy*
```{r}
table(kld_train_pred, train_feat$labels)
```
```{r}
table(kld_test_pred, test_feat$labels)
```

```{r}
KLD_table <- data.frame("Training_Accuracy" = rep(NA,5), "Test_Accuracy" = rep(NA,5))
row.names(KLD_table) <- c("Business", "Entertainment", "Politics", "Sport","Technology")
KLD_table[1,1] <- 349/357;KLD_table[1,2] <- 146/153
KLD_table[2,1] <- 232/270;KLD_table[2,2] <- 99/116
KLD_table[3,1] <- 232/291;KLD_table[3,2] <- 100/126
KLD_table[4,1] <- 320/357;KLD_table[4,2] <- 140/154
KLD_table[5,1] <- 250/280;KLD_table[5,2] <- 100/121
kable(KLD_table, caption = "Summary of KLD Classification Performance")
```


*SVM accuracy*
```{r}
table(fit_svm@fitted, train_feat$labels)
```
```{r}
table(svm_test_pred, test_feat$labels)
```

```{r}
SVM_table <- data.frame("Training_Accuracy" = rep(NA,5), "Test_Accuracy" = rep(NA,5))
row.names(SVM_table) <- c("Business", "Entertainment", "Politics", "Sport","Technology")
SVM_table[1,1] <- 354/357;SVM_table[1,2] <- 148/153
SVM_table[2,1] <- 267/270;SVM_table[2,2] <- 101/116
SVM_table[3,1] <- 287/291;SVM_table[3,2] <- 110/126
SVM_table[4,1] <- 356/357;SVM_table[4,2] <- 153/154
SVM_table[5,1] <- 247/280;SVM_table[5,2] <- 110/121
kable(SVM_table, caption = "Summary of SVM Classification Performance")
```

*All accuracy*
```{r all results}

performance_table <- data.frame("Value" = rep(NA,4))
row.names(performance_table) <- c("SVM training error", "SVM test error", "KLD training error", "KLD test error")
performance_table[1,1] <- fit_svm@error
performance_table[2,1] <- svm_test_error
performance_table[3,1] <- kld_train_err
performance_table[4,1] <- kld_test_err 

kable(performance_table, caption = "Summary of Nonparametric Text Classification Performance")

```


